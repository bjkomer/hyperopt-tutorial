{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This example shows how to use hyperopt to pick parameters for a MLP classifier on the MNIST handwritten digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "# Scale the images to be between 0 and 1\n",
    "X = mnist.data / 255.\n",
    "y = mnist.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define an objective function to minimize\n",
    "# The classifier will be created, trained, and scored within this function\n",
    "def objective(args):\n",
    "    \n",
    "    # Build a classifier based on the parameters chosen\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(int(args['layer_size']),), max_iter=10,\n",
    "                        alpha=args['alpha'], algorithm=args['algorithm'], tol=1e-4, \n",
    "                        random_state=1, activation=args['activation'], \n",
    "                        learning_rate_init=args['learning_rate']\n",
    "                       )\n",
    "    \n",
    "    # Fit the classifier to the training data\n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "    #NOTE: Normally you should use a separate 'validation' set here\n",
    "    #      and have a 'test' set that is only used on the final classifier\n",
    "    #      once parameters have been selected, the final classifier can be\n",
    "    #      retrained on both the 'training' and 'validation' sets\n",
    "    loss = -mlp.score(X_test, y_test)\n",
    "    \n",
    "    # Must return loss and status, any additional information can also be saved here.\n",
    "    # In this example the fully trained model is also returned\n",
    "    return {'loss': loss, 'status': STATUS_OK, 'model':mlp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the parameter space to search over\n",
    "# In this case the objective function is expecting a single dictionary argument, \n",
    "# so the space variable is set up to match that\n",
    "space = {'layer_size':hp.quniform('layer_size', 25, 100, 1),\n",
    "         'alpha':hp.lognormal('alpha', mu=np.log(1e-4), sigma=1),\n",
    "         'algorithm':hp.choice('algorithm', ['l-bfgs', 'sgd', 'adam']),\n",
    "         'activation':hp.choice('activation', ['logistic', 'tanh', 'relu']),\n",
    "         #'learning_rate':hp.uniform('learning_rate', low=0.001, high=0.999),\n",
    "         'learning_rate':hp.loguniform('learning_rate', low=np.log(1e-4), high=np.log(1.)),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a Trials object to store results of each evaluation\n",
    "trials = Trials()\n",
    "\n",
    "# Run the search for the specified number of evaluations\n",
    "best = fmin(objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            trials=trials,\n",
    "            max_evals=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.985050\n",
      "Testing Accuracy: 0.972200\n"
     ]
    }
   ],
   "source": [
    "# Get the trained model from the best trial\n",
    "best_model = trials.best_trial['result']['model']\n",
    "\n",
    "# Compute the training and testing scores on this model\n",
    "print(\"Training Accuracy: %f\" % best_model.score(X_train, y_train))\n",
    "print(\"Testing Accuracy: %f\" % best_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 0,\n",
       " 'algorithm': 1,\n",
       " 'alpha': 5.7616756046266688e-05,\n",
       " 'layer_size': 92.0,\n",
       " 'learning_rate': 0.36964613992731787}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'book_time': datetime.datetime(2016, 5, 24, 21, 9, 31, 41000),\n",
       " 'exp_key': None,\n",
       " 'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
       "  'idxs': {'activation': [3],\n",
       "   'algorithm': [3],\n",
       "   'alpha': [3],\n",
       "   'layer_size': [3],\n",
       "   'learning_rate': [3]},\n",
       "  'tid': 3,\n",
       "  'vals': {'activation': [2],\n",
       "   'algorithm': [1],\n",
       "   'alpha': [0.00016887477537756929],\n",
       "   'layer_size': [45.0],\n",
       "   'learning_rate': [0.085753033052009875]},\n",
       "  'workdir': None},\n",
       " 'owner': None,\n",
       " 'refresh_time': datetime.datetime(2016, 5, 24, 21, 9, 37, 523000),\n",
       " 'result': {'loss': -0.9722,\n",
       "  'model': MLPClassifier(activation='relu', algorithm='sgd', alpha=0.000168874775378,\n",
       "         batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
       "         epsilon=1e-08, hidden_layer_sizes=(45,), learning_rate='constant',\n",
       "         learning_rate_init=0.085753033052, max_iter=10, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "         tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "         warm_start=False),\n",
       "  'status': 'ok'},\n",
       " 'spec': None,\n",
       " 'state': 2,\n",
       " 'tid': 3,\n",
       " 'version': 0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': -0.9315,\n",
       "  'model': MLPClassifier(activation='relu', algorithm='sgd', alpha=9.14078888052e-05,\n",
       "         batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
       "         epsilon=1e-08, hidden_layer_sizes=(73,), learning_rate='constant',\n",
       "         learning_rate_init=0.00284028790736, max_iter=10, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "         tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "         warm_start=False),\n",
       "  'status': 'ok'},\n",
       " {'loss': -0.9395,\n",
       "  'model': MLPClassifier(activation='tanh', algorithm='adam', alpha=2.04779917725e-05,\n",
       "         batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
       "         epsilon=1e-08, hidden_layer_sizes=(53,), learning_rate='constant',\n",
       "         learning_rate_init=0.000204837302568, max_iter=10, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "         tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "         warm_start=False),\n",
       "  'status': 'ok'},\n",
       " {'loss': -0.8292,\n",
       "  'model': MLPClassifier(activation='tanh', algorithm='sgd', alpha=3.28647154721e-05,\n",
       "         batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
       "         epsilon=1e-08, hidden_layer_sizes=(71,), learning_rate='constant',\n",
       "         learning_rate_init=0.0001138759524, max_iter=10, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "         tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "         warm_start=False),\n",
       "  'status': 'ok'},\n",
       " {'loss': -0.9722,\n",
       "  'model': MLPClassifier(activation='relu', algorithm='sgd', alpha=0.000168874775378,\n",
       "         batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
       "         epsilon=1e-08, hidden_layer_sizes=(45,), learning_rate='constant',\n",
       "         learning_rate_init=0.085753033052, max_iter=10, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "         tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "         warm_start=False),\n",
       "  'status': 'ok'},\n",
       " {'loss': -0.9625,\n",
       "  'model': MLPClassifier(activation='tanh', algorithm='adam', alpha=0.000251449202544,\n",
       "         batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
       "         epsilon=1e-08, hidden_layer_sizes=(45,), learning_rate='constant',\n",
       "         learning_rate_init=0.00943514063958, max_iter=10, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "         tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "         warm_start=False),\n",
       "  'status': 'ok'},\n",
       " {'loss': -0.9011,\n",
       "  'model': MLPClassifier(activation='tanh', algorithm='l-bfgs', alpha=0.000124507585017,\n",
       "         batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
       "         epsilon=1e-08, hidden_layer_sizes=(74,), learning_rate='constant',\n",
       "         learning_rate_init=0.0383119095425, max_iter=10, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "         tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "         warm_start=False),\n",
       "  'status': 'ok'},\n",
       " {'loss': -0.9694,\n",
       "  'model': MLPClassifier(activation='tanh', algorithm='sgd', alpha=0.000108412127024,\n",
       "         batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
       "         epsilon=1e-08, hidden_layer_sizes=(68,), learning_rate='constant',\n",
       "         learning_rate_init=0.419230978839, max_iter=10, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "         tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "         warm_start=False),\n",
       "  'status': 'ok'},\n",
       " {'loss': -0.9529,\n",
       "  'model': MLPClassifier(activation='logistic', algorithm='adam',\n",
       "         alpha=8.97228483499e-05, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(31,), learning_rate='constant',\n",
       "         learning_rate_init=0.00166167130506, max_iter=10, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "         tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "         warm_start=False),\n",
       "  'status': 'ok'},\n",
       " {'loss': -0.9672,\n",
       "  'model': MLPClassifier(activation='logistic', algorithm='adam',\n",
       "         alpha=7.99682385727e-05, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(64,), learning_rate='constant',\n",
       "         learning_rate_init=0.0231714277565, max_iter=10, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "         tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "         warm_start=False),\n",
       "  'status': 'ok'},\n",
       " {'loss': -0.8683,\n",
       "  'model': MLPClassifier(activation='relu', algorithm='l-bfgs', alpha=2.3479840138e-05,\n",
       "         batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
       "         epsilon=1e-08, hidden_layer_sizes=(71,), learning_rate='constant',\n",
       "         learning_rate_init=0.000170949689551, max_iter=10, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "         tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "         warm_start=False),\n",
       "  'status': 'ok'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
